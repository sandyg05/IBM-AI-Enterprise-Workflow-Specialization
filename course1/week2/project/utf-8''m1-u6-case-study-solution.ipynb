{
 "cells": [
  {
   "attachments": {
    "ibm-cloud.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ibm-cloud.png](attachment:ibm-cloud.png)\n",
    "\n",
    "# Case Study - Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this case study is to put into practice the important concepts from module 1.  We will go through the basic process that begins with refining the business opportunity and ensuring that it is articulated using a scientific thought process.\n",
    "\n",
    "You will be gathering data from several provided sources, staging it for quality assurance and saving it in a target destination that is most appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study overall objectives\n",
    "\n",
    "1. Gather all relevant data from the sources of provided data\n",
    "2. Implement several checks for quality assurance \n",
    "3. Take the initial steps towards automation of the ingestion pipeline\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Download this notebook and open it locally using a Jupyter server. Alternatively you may use Watson Studio.  To make using Watson Studio easier we have provided a zip archive file containing the files needed to complete this case study in Watson Studio.\n",
    "\n",
    "**You will need the following files to complete this case study**\n",
    "\n",
    "* m1-u6-case-study.ipynb\n",
    "* m1-u6-case-study-solution.ipynb\n",
    "* aavail-customers.db\n",
    "* aavail-steams.csv\n",
    "\n",
    "1. Fill in all of the places in this notebook marked with ***YOUR CODE HERE*** or ***YOUR ANSWER HERE***\n",
    "2. When you have finished the case study there will be a short quiz\n",
    "\n",
    "You may review the rest of this content as part of the notebook, but once you are ready to get started be ensure that you are working with a *live* version either as part of Watson Studio or locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "The data you will be sourcing from is contained in two sources.\n",
    "\n",
    "1. A database ([SQLite](https://www.sqlite.org/index.html)) of `customer` data\n",
    "2. A [CSV file](https://en.wikipedia.org/wiki/Comma-separated_values) of `stream` level data\n",
    "\n",
    "   >You will create a simple data pipeline that\n",
    "   (1) simplifies the data for future analysis\n",
    "   (2) performs quality assurance checks.\n",
    "\n",
    "The process of building *the data ingestion pipeline* entails extracting data, transforming it, and loading it into an appropriate data storage technology.  When constructing a pipeline it is important to keep in mind that they generally process data in batches.  For example, data may be compiled during the day and the batch could be processed during the night.  The data pipeline may also be optimized to execute as a streaming computation (i.e., every event is handled as it occurs)."
   ]
  },
  {
   "attachments": {
    "aavail-schema.svg": {
     "image/svg+xml": [
      "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"
     ]
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Gathering the data\n",
    "\n",
    "The following is an [Entity Relationship Diagram (ERD)](https://en.wikipedia.org/wiki/Entity%E2%80%93relationship_model) that details the tables and contents of the database.\n",
    "\n",
    "![aavail-schema.svg](attachment:aavail-schema.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all the imports you will need for this case study\n",
    "import os\n",
    "from datetime import datetime\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "## specify the directory you saved the data in\n",
    "data_dir = os.path.join(\"..\",\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the data exist in a database.  You can connect to is using the `sqlite3` Python package with the function shown below.  Note that is is good practice to wrap your connect functions in a [try-except statement](https://docs.python.org/3/tutorial/errors.html) to cleanly handle exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_db(file_path):\n",
    "    try:\n",
    "        conn = sqlite3.connect(file_path)\n",
    "        print(\"...successfully connected to db\\n\")\n",
    "    except Error as e:\n",
    "        print(\"...unsuccessful connection\\n\",e)\n",
    "    \n",
    "    return(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...successfully connected to db\n",
      "\n",
      "['CUSTOMER', 'INVOICE', 'INVOICE_ITEM', 'COUNTRY']\n"
     ]
    }
   ],
   "source": [
    "## make the connection to the database\n",
    "conn = connect_db(os.path.join(data_dir,\"aavail-customers.db\"))\n",
    "\n",
    "## print the table names\n",
    "tables = [t[0] for t in conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")]\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1:\n",
    "\n",
    "**extract the relevant data from the DB**\n",
    "\n",
    "Query the database and extract the following data into a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).\n",
    " \n",
    "* Customer ID (integer)\n",
    "* Last name\n",
    "* First name\n",
    "* DOB\n",
    "* City\n",
    "* State\n",
    "* Country (the name NOT the country_id)\n",
    "* Gender\n",
    "\n",
    "Remember that that SQL is case-insensitive, but it is traditional to use ALL CAPS for SQL keywords. It is also a convention to end SQL statements with a semi-colon.  \n",
    "\n",
    "#### Resources\n",
    "\n",
    "* [W3 schools SQL tutorial](https://www.w3schools.com/sql)\n",
    "* [W3 schools SQL joins](https://www.w3schools.com/sql/sql_join.asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>last_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>DOB</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Todd</td>\n",
       "      <td>Kasen</td>\n",
       "      <td>07/30/98</td>\n",
       "      <td>Rock Hill</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>united_states</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Garza</td>\n",
       "      <td>Ensley</td>\n",
       "      <td>04/12/89</td>\n",
       "      <td>singapore</td>\n",
       "      <td>None</td>\n",
       "      <td>singapore</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Carey</td>\n",
       "      <td>Lillian</td>\n",
       "      <td>09/12/97</td>\n",
       "      <td>Auburn</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>united_states</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Christensen</td>\n",
       "      <td>Beau</td>\n",
       "      <td>01/28/99</td>\n",
       "      <td>Hempstead</td>\n",
       "      <td>New York</td>\n",
       "      <td>united_states</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Gibson</td>\n",
       "      <td>Ernesto</td>\n",
       "      <td>03/23/98</td>\n",
       "      <td>singapore</td>\n",
       "      <td>None</td>\n",
       "      <td>singapore</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id    last_name first_name       DOB       city           state  \\\n",
       "0            1         Todd      Kasen  07/30/98  Rock Hill  South Carolina   \n",
       "1            2        Garza     Ensley  04/12/89  singapore            None   \n",
       "2            3        Carey    Lillian  09/12/97     Auburn         Alabama   \n",
       "3            4  Christensen       Beau  01/28/99  Hempstead        New York   \n",
       "4            5       Gibson    Ernesto  03/23/98  singapore            None   \n",
       "\n",
       "         country gender  \n",
       "0  united_states      m  \n",
       "1      singapore      f  \n",
       "2  united_states      f  \n",
       "3  united_states      m  \n",
       "4      singapore      m  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "query = \"\"\"\n",
    "SELECT cu.customer_id, cu.last_name, cu.first_name, cu.DOB,\n",
    "       cu.city, cu.state, co.country_name, cu.gender\n",
    "FROM CUSTOMER cu\n",
    "INNER JOIN COUNTRY co\n",
    "ON cu.country_id = co.country_id;\n",
    "\"\"\"\n",
    "\n",
    "_data = [d for d in conn.execute(query)]\n",
    "columns = [\"customer_id\",\"last_name\",\"first_name\",\"DOB\",\"city\",\"state\",\"country\",\"gender\"]\n",
    "df_db = pd.DataFrame(_data,columns=columns)\n",
    "df_db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 2:\n",
    "\n",
    "**Extract the relevant data from the CSV file**\n",
    "\n",
    "For each ```customer_id``` determine if a customer has stopped their subscription or not and save it in a dictionary or another data container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>stream_id</th>\n",
       "      <th>date</th>\n",
       "      <th>invoice_item_id</th>\n",
       "      <th>subscription_stopped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>2018-10-21</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1343.0</td>\n",
       "      <td>2018-10-23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1756.0</td>\n",
       "      <td>2018-11-05</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>2018-11-06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1324.0</td>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  stream_id        date  invoice_item_id  subscription_stopped\n",
       "0            1     1420.0  2018-10-21              2.0                     0\n",
       "1            1     1343.0  2018-10-23              2.0                     0\n",
       "2            1     1756.0  2018-11-05              2.0                     0\n",
       "3            1     1250.0  2018-11-06              2.0                     0\n",
       "4            1     1324.0  2018-11-12              2.0                     0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_streams = pd.read_csv(os.path.join(data_dir,r\"aavail-streams.csv\"))\n",
    "df_streams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>is_subscriber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  is_subscriber\n",
       "0            1              1\n",
       "1            2              0\n",
       "2            3              0\n",
       "3            4              1\n",
       "4            5              1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "customer_ids = df_streams['customer_id'].values\n",
    "unique_ids = np.unique(df_streams['customer_id'].values)\n",
    "streams = df_streams['subscription_stopped'].values\n",
    "has_churned = [0 if streams[customer_ids==uid].max() > 0 else 1 for uid in unique_ids]\n",
    "df_churn = pd.DataFrame({\"customer_id\": unique_ids,\"is_subscriber\": has_churned})\n",
    "df_churn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Checks for quality assurance\n",
    "\n",
    "Sometimes it is known in advance which types of data integrity issues to expect, but other times it is during the Exploratory Data Analysis (EDA) process that these issues are identified.  After extracting data it is important to include checks for quality assurance even on the first pass through the AI workflow.  Here you will combine the data into a single structure and provide a couple checks for quality assurance.\n",
    "\n",
    "### QUESTION 3: \n",
    "\n",
    "**Implement checks for quality assurance**\n",
    "\n",
    "1. Remove any repeat customers based on ```customer_id```\n",
    "2. Remove stream data that do not have an associated ```stream_id```\n",
    "3. Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning Summary\n",
      "-----------------------------------\n",
      "Removed 7 duplicate rows\n",
      "Removed 1164 missing stream ids\n",
      "\n",
      "Missing Value Summary\n",
      "-----------------------------------\n",
      "\n",
      "df_db\n",
      "---------------\n",
      "customer_id      0\n",
      "last_name        0\n",
      "first_name       0\n",
      "DOB              0\n",
      "city             0\n",
      "state          300\n",
      "country          0\n",
      "gender           0\n",
      "dtype: int64\n",
      "\n",
      "df_streams\n",
      "---------------\n",
      "customer_id             0\n",
      "stream_id               0\n",
      "date                    0\n",
      "invoice_item_id         0\n",
      "subscription_stopped    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "print(\"\\nCleaning Summary\\n{}\".format(\"-\"*35))\n",
    "duplicate_rows = df_db.duplicated()\n",
    "if True in duplicate_rows:\n",
    "    df_db = df_db[~duplicate_rows]\n",
    "print(\"Removed {} duplicate rows\".format(np.where(duplicate_rows==True)[0].size))\n",
    "\n",
    "missing_stream_ids = np.isnan(df_streams['stream_id'])    \n",
    "if True in missing_stream_ids:\n",
    "    df_streams = df_streams[~missing_stream_ids]\n",
    "print(\"Removed {} missing stream ids\".format(np.where(missing_stream_ids==True)[0].size))\n",
    "\n",
    "print(\"\\nMissing Value Summary\\n{}\".format(\"-\"*35))\n",
    "print(\"\\ndf_db\\n{}\".format(\"-\"*15))\n",
    "print(df_db.isnull().sum(axis = 0))\n",
    "print(\"\\ndf_streams\\n{}\".format(\"-\"*15))\n",
    "print(df_streams.isnull().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 4: \n",
    "\n",
    "**combine the data into a single data structure**\n",
    "\n",
    "For this example, the two most convenient structures for this task are Pandas dataframes and NumPy arrays.  At a minimum ensure that your structure accommodates the following.\n",
    "\n",
    "1. A column for `customer_id`\n",
    "2. A column for `country`\n",
    "3. A column for ```age``` that is created from ```DOB```\n",
    "4. A column ```customer_name``` that is created from ```first_name``` and ```last_name```\n",
    "5. A column to indicate churn called ```is_subscriber```\n",
    "7. A column that indicates ```subscriber_type``` that comes from ```invoice_item```\n",
    "6. A column to indicate the total ```num_streams```\n",
    "\n",
    "> HINT: For the subscriber type use the most frequent invoice_item\n",
    "\n",
    "#### Resources\n",
    "\n",
    "* [Python's datetime library](https://docs.python.org/3/library/datetime.html)\n",
    "* [NumPy's datetime data type](https://docs.scipy.org/doc/numpy/reference/arrays.datetime.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>is_subscriber</th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>subscriber_type</th>\n",
       "      <th>num_streams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>united_states</td>\n",
       "      <td>21</td>\n",
       "      <td>Kasen Todd</td>\n",
       "      <td>aavail_premium</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>singapore</td>\n",
       "      <td>30</td>\n",
       "      <td>Ensley Garza</td>\n",
       "      <td>aavail_unlimited</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>united_states</td>\n",
       "      <td>21</td>\n",
       "      <td>Lillian Carey</td>\n",
       "      <td>aavail_premium</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>united_states</td>\n",
       "      <td>20</td>\n",
       "      <td>Beau Christensen</td>\n",
       "      <td>aavail_basic</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>singapore</td>\n",
       "      <td>21</td>\n",
       "      <td>Ernesto Gibson</td>\n",
       "      <td>aavail_premium</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  is_subscriber        country  age     customer_name  \\\n",
       "0            1              1  united_states   21        Kasen Todd   \n",
       "1            2              0      singapore   30      Ensley Garza   \n",
       "2            3              0  united_states   21     Lillian Carey   \n",
       "3            4              1  united_states   20  Beau Christensen   \n",
       "4            5              1      singapore   21    Ernesto Gibson   \n",
       "\n",
       "    subscriber_type  num_streams  \n",
       "0    aavail_premium           23  \n",
       "1  aavail_unlimited           12  \n",
       "2    aavail_premium           22  \n",
       "3      aavail_basic           19  \n",
       "4    aavail_premium           23  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "df_clean = df_churn.copy()\n",
    "df_clean = df_clean[np.in1d(df_clean['customer_id'].values,df_db['customer_id'].values)]\n",
    "unique_ids = df_clean['customer_id'].values\n",
    "\n",
    "## ensure we are working with correctly ordered customer_ids df_db\n",
    "if not np.array_equal(df_clean['customer_id'],df_db['customer_id']): \n",
    "    raise Exception(\"indexes are out of order or unmatched---needs to fix\")\n",
    "\n",
    "## query the db t create a invoice item map\n",
    "query = \"\"\"\n",
    "SELECT i.invoice_item_id, i.invoice_item\n",
    "FROM INVOICE_ITEM i;\n",
    "\"\"\"\n",
    "\n",
    "## variables for new df creation\n",
    "invoice_item_map = {d[0]:d[1] for d in conn.execute(query)}\n",
    "streams_stopped = df_streams['subscription_stopped'].values\n",
    "streams_cid = df_streams['customer_id'].values\n",
    "streams_iid = df_streams['invoice_item_id'].values\n",
    "subscriber_invoice_mode = [stats.mode(streams_iid[streams_cid==uid])[0][0] for uid in unique_ids]\n",
    "\n",
    "## create the new df\n",
    "df_clean['country'] = df_db['country']\n",
    "df_clean['age'] = np.datetime64('today') - df_db['DOB'].astype('datetime64')\n",
    "df_clean['customer_name'] = df_db['first_name'] + \" \" + df_db['last_name']\n",
    "df_clean['subscriber_type'] = [invoice_item_map[int(sim)] for sim in subscriber_invoice_mode]\n",
    "df_clean['num_streams'] = [streams_stopped[streams_cid==uid].size for uid in unique_ids]\n",
    "\n",
    "## convert age to days\n",
    "df_clean['age'] = [a.astype('timedelta64[Y]').astype(int) for a in df_clean['age'].values]\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Automating the process\n",
    "\n",
    "To ensure that you code can be used to automate this process.  First you will save you dataframe or numpy array as a CSV file.  \n",
    "\n",
    "### QUESTION 5:\n",
    "\n",
    "**Take the initial steps towards automation**\n",
    "\n",
    "1. Save your cleaned, combined data as a CSV file.\n",
    "2. From the code above create a function or class that performs all of the steps given a database file and a streams CSV file.\n",
    "3. Run the function in batches and write a check to ensure you got the same result that you did in the code above.\n",
    "\n",
    "There will be some logic involved to ensure that you do not write the same data twice to the target CSV file.\n",
    "\n",
    "Shown below is some code that will split your streams file into two batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code to split the streams csv into batches\n",
    "df_all = pd.read_csv(os.path.join(data_dir,\"aavail-streams.csv\"))\n",
    "half = int(round(df_all.shape[0] * 0.5))\n",
    "df_part1 = df_all[:half]\n",
    "df_part2 = df_all[half:]\n",
    "df_part1.to_csv(os.path.join(data_dir,\"aavail-streams-1.csv\"),index=False)\n",
    "df_part2.to_csv(os.path.join(data_dir,\"aavail-streams-2.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to save your function as a file.  The following cell demonstrates how to do this from within a notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing aavail-data-ingestor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile aavail-data-ingestor.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import getopt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "DATA_DIR = os.path.join(\"..\",\"data\")\n",
    "\n",
    "def connect_db(file_path):\n",
    "    \"\"\"\n",
    "    function to connection to aavail database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(file_path)\n",
    "        print(\"...successfully connected to db\")\n",
    "    except Error as e:\n",
    "        print(\"...unsuccessful connection\",e)\n",
    "    \n",
    "    return(conn)\n",
    "\n",
    "def ingest_db_data(conn):\n",
    "    \"\"\"\n",
    "    load and clean the db data\n",
    "    \"\"\"\n",
    "    \n",
    "    query = \"\"\"\n",
    "            SELECT cu.customer_id, cu.last_name, cu.first_name, cu.DOB,\n",
    "            cu.city, cu.state, co.country_name, cu.gender\n",
    "            FROM CUSTOMER cu\n",
    "            INNER JOIN COUNTRY co\n",
    "            ON cu.country_id = co.country_id;\n",
    "            \"\"\"\n",
    "    _data = [d for d in conn.execute(query)]\n",
    "    columns = [\"customer_id\",\"last_name\",\"first_name\",\"DOB\",\"city\",\"state\",\"country\",\"gender\"]\n",
    "    df_db = pd.DataFrame(_data,columns=columns)\n",
    "    duplicate_rows = df_db.duplicated()\n",
    "    if True in duplicate_rows:\n",
    "        df_db = df_db[~duplicate_rows]\n",
    "        df_db.reset_index()\n",
    "    print(\"... removed {} duplicate rows in db data\".format(np.where(duplicate_rows==True)[0].size))\n",
    "    return(df_db)\n",
    "\n",
    "def ingest_stream_data(file_path):\n",
    "    \"\"\"\n",
    "    load and clean the stream data\n",
    "    \"\"\"\n",
    "    \n",
    "    df_streams = pd.read_csv(file_path)\n",
    "    customer_ids = df_streams['customer_id'].values\n",
    "    unique_ids = np.unique(df_streams['customer_id'].values)\n",
    "    streams = df_streams['subscription_stopped'].values\n",
    "    has_churned = [0 if streams[customer_ids==uid].max() > 0 else 1 for uid in unique_ids]\n",
    "    df_churn = pd.DataFrame({\"customer_id\": unique_ids,\"is_subscriber\": has_churned})\n",
    "    \n",
    "    missing_stream_ids = np.isnan(df_streams['stream_id'])    \n",
    "    if True in missing_stream_ids:\n",
    "        df_streams = df_streams[~missing_stream_ids]\n",
    "        df_streams.reset_index()\n",
    "    print(\"... removed {} missing stream ids\".format(np.where(missing_stream_ids==True)[0].size))\n",
    "    \n",
    "    return(df_streams,df_churn)\n",
    "\n",
    "def process_dataframes(df_db,df_streams,df_churn,conn):\n",
    "    \"\"\"\n",
    "    add data to target csv\n",
    "    \"\"\"\n",
    "\n",
    "    df_clean = df_churn.copy()\n",
    "    df_db = df_db[np.in1d(df_db['customer_id'].values,df_clean['customer_id'].values)]\n",
    "    df_db.reset_index()\n",
    "    unique_ids = df_clean['customer_id'].values\n",
    "\n",
    "    ## ensure we are working with correctly ordered customer_ids df_db\n",
    "    if not np.array_equal(df_clean['customer_id'],df_db['customer_id']):\n",
    "        raise Exception(\"indexes are out of order or unmatched---needs to fix\")\n",
    "\n",
    "    ## query the db t create a invoice item map\n",
    "    query = \"\"\"\n",
    "    SELECT i.invoice_item_id, i.invoice_item\n",
    "    FROM INVOICE_ITEM i;\n",
    "    \"\"\"\n",
    "\n",
    "    ## variables for new df creation\n",
    "    invoice_item_map = {d[0]:d[1] for d in conn.execute(query)}\n",
    "    streams_stopped = df_streams['subscription_stopped'].values\n",
    "    streams_cid = df_streams['customer_id'].values\n",
    "    streams_iid = df_streams['invoice_item_id'].values\n",
    "    subscriber_invoice_mode = [stats.mode(streams_iid[streams_cid==uid])[0][0] for uid in unique_ids]\n",
    "\n",
    "    ## create the new df\n",
    "    df_clean['country'] = df_db['country']\n",
    "    df_clean['age'] = np.datetime64('today') - df_db['DOB'].astype('datetime64')\n",
    "    df_clean['age'] = [a.astype('timedelta64[Y]').astype(int) for a in df_clean['age'].values]\n",
    "    df_clean['customer_name'] = df_db['first_name'] + \" \" + df_db['last_name']\n",
    "    df_clean['subscriber_type'] = [invoice_item_map[int(sim)] for sim in subscriber_invoice_mode]\n",
    "    df_clean['num_streams'] = [streams_stopped[streams_cid==uid].size for uid in unique_ids]\n",
    "    \n",
    "    return(df_clean)\n",
    "    \n",
    "def update_target(target_file,df_clean,overwrite=False):\n",
    "    \"\"\"\n",
    "    update line by line in case data are large\n",
    "    \"\"\"\n",
    "\n",
    "    if overwrite or not os.path.exists(target_file):\n",
    "        df_clean.to_csv(target_file,index=False)   \n",
    "    else:\n",
    "        df_target = pd.read_csv(target_file)\n",
    "        df_target.to_csv(target_file, mode='a',index=False)\n",
    "         \n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    ## collect args\n",
    "    arg_string = \"%s -d db_filepath -s streams_filepath\"%sys.argv[0]\n",
    "    try:\n",
    "        optlist, args = getopt.getopt(sys.argv[1:],'d:s:')\n",
    "    except getopt.GetoptError:\n",
    "        print(getopt.GetoptError)\n",
    "        raise Exception(arg_string)\n",
    "\n",
    "    ## handle args\n",
    "    streams_file = None\n",
    "    db_file = None\n",
    "    for o, a in optlist:\n",
    "        if o == '-d':\n",
    "            db_file = a\n",
    "        if o == '-s':\n",
    "            streams_file = a\n",
    "    streams_file = os.path.join(DATA_DIR,streams_file)\n",
    "    db_file = os.path.join(DATA_DIR,db_file)\n",
    "    target_file = os.path.join(DATA_DIR,\"aavail-target.csv\")\n",
    "    \n",
    "    ## make the connection to the database\n",
    "    conn = connect_db(db_file)\n",
    "\n",
    "    ## ingest data base data\n",
    "    df_db = ingest_db_data(conn)\n",
    "    df_streams, df_churn = ingest_stream_data(streams_file)\n",
    "    df_clean = process_dataframes(df_db, df_streams, df_churn, conn)\n",
    "    \n",
    "    ## write\n",
    "    update_target(target_file,df_clean,overwrite=False)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to be able to pass the file names to your function without hardcoding them into the script itself.  This is an important step towards automation.  Here are the two libraries commonly used to accomplish this in Python.\n",
    "\n",
    "* [getopt](https://docs.python.org/3/library/getopt.html)\n",
    "* [argparse](https://docs.python.org/3/library/argparse.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may run the script you just created from the commandline directly or from within this notebook using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...successfully connected to db\n",
      "... removed 7 duplicate rows in db data\n",
      "... removed 1164 missing stream ids\n",
      "   customer_id  is_subscriber  ...   subscriber_type  num_streams\n",
      "0            1              1  ...    aavail_premium           23\n",
      "1            2              0  ...  aavail_unlimited           12\n",
      "2            3              0  ...    aavail_premium           22\n",
      "3            4              1  ...      aavail_basic           19\n",
      "4            5              1  ...    aavail_premium           23\n",
      "\n",
      "[5 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "!python aavail-data-ingestor.py aavail-customers.db aavail-streams-1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the script once for each batch that you created and then load both the original and batch versions back into the notebook to check that they are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...successfully connected to db\n",
      "... removed 7 duplicate rows in db data\n",
      "... removed 1164 missing stream ids\n",
      "done\n",
      "    1001 ../data/aavail-target.csv\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "!rm ../data/aavail-target.csv\n",
    "!python aavail-data-ingestor.py -d aavail-customers.db -s aavail-streams.csv\n",
    "!wc -l ../data/aavail-target.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...successfully connected to db\n",
      "... removed 7 duplicate rows in db data\n",
      "... removed 577 missing stream ids\n",
      "done\n",
      "     507 ../data/aavail-target.csv\n",
      "...successfully connected to db\n",
      "... removed 7 duplicate rows in db data\n",
      "... removed 587 missing stream ids\n",
      "done\n",
      "    1014 ../data/aavail-target.csv\n"
     ]
    }
   ],
   "source": [
    "!rm ../data/aavail-target.csv\n",
    "!python aavail-data-ingestor.py -d aavail-customers.db -s aavail-streams-1.csv\n",
    "!wc -l ../data/aavail-target.csv\n",
    "!python aavail-data-ingestor.py -d aavail-customers.db -s aavail-streams-2.csv\n",
    "!wc -l ../data/aavail-target.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 6:\n",
    "\n",
    "**How can you improve the process?**\n",
    "\n",
    "In paragraph form or using bullets write down some of the ways that you could improve this pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
